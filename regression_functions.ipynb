{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score \n",
    "from sklearn.cross_validation import KFold\n",
    "import sklearn.preprocessing as pp\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.learning_curve import learning_curve\n",
    "import sklearn.decomposition\n",
    "import sklearn.ensemble as sk\n",
    "from sklearn import linear_model\n",
    "\n",
    "import random\n",
    "import sys\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Declare whether to process raw or filtered data.\n",
    "def declare_filt_or_raw_dataset(which_data):\n",
    "    if which_data == 0:\n",
    "        ref_column = 'O3_ppb'\n",
    "        leave_out_pod = 'pod_o3_smooth'\n",
    "        pod_ozone = 'e2v03'\n",
    "    else:\n",
    "        ref_column = 'ref_o3_smooth'\n",
    "        leave_out_pod = 'e2v03'\n",
    "        pod_ozone = 'pod_o3_smooth'\n",
    "    return ref_column, leave_out_pod, pod_ozone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####Define a function that makes numpy arrays out of the training and holdout data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_numpy_arrays_for_tr_and_holdout(features, df_T, df_CV, ref_column):\n",
    "    X_T = df_T[features].values\n",
    "    X_CV = df_CV[features].values\n",
    "    y_T = df_T[ref_column].values\n",
    "    y_CV = df_CV[ref_column].values\n",
    "    return X_T, y_T, X_CV, y_CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Add a 'day' column to the dataframe, and separate the data into training and holdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_day_sep_tr_and_holdout(df, ref_column):\n",
    "    #create a 'day' column in the dataframe by mapping the index column\n",
    "    df['day'] = df.index.map(lambda dt: str(dt.month) + '-' + str(dt.day))\n",
    "    days = df['day'].unique()\n",
    " \n",
    "    \n",
    "    num_days = np.argmax(days)\n",
    "    days_tr = days[:num_days-1]\n",
    "    df_tr = df.loc[df['day'] < days[num_days-1]]\n",
    "    df_hold = df.loc[df['day'].isin([days[num_days], days[num_days-1]])]\n",
    "    \n",
    "    return df_tr, df_hold, days_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Scale the features and add a 'day' column to the dataframe.\n",
    "def scale_features_and_create_day_column(df, ref_column):\n",
    "    df_scaled = df.copy()\n",
    "    #drop the day column from df_scaled\n",
    "    df_scaled.drop('day', axis=1, inplace=True)\n",
    "    \n",
    "    features = list(df_scaled.ix[:,1:len(df.columns)])\n",
    "    #Center feature values around zero and make them all have variance on the same order.\n",
    "    df_scaled = df_scaled[features].apply(lambda x: pp.scale(x))\n",
    "    df_sc = pd.concat([df_scaled, df[ref_column]], axis = 1)\n",
    "    \n",
    "    #add the 'day' column back in\n",
    "    df_sc['day'] = df_sc.index.map(lambda dt: str(dt.month) + '-' + str(dt.day))\n",
    "    \n",
    "    return df_sc, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define a custom cross-validation function.\n",
    "def create_custom_cv(df):\n",
    "    labels = df['day'].values\n",
    "    lol = cross_validation.LeaveOneLabelOut(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####Declare a neutral fitting function.\n",
    "def fitting_func(model, X_T, y_T, X_CV, y_CV):    \n",
    "    #fit a linear regression on the training data\n",
    "    model.fit(X_T, y_T)   \n",
    "    #find the normalized MSE for the training and holdout data\n",
    "    return np.mean((y_CV - model.predict(X_CV))**2), np.mean((y_T - model.predict(X_T))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####Define a function that loops through all of the days (CV by day), and computes MSE.\n",
    "def cross_validation_by_day(model, features, df, days, ref_column):\n",
    "\n",
    "    #initialize the holdout and training MSE\n",
    "    day_date = []\n",
    "    MSE_CV = [] \n",
    "    MSE_T = []\n",
    "    #Calculate the training and holdout RSS for each step.\n",
    "    #take the mean MSE for all of the possible holdout days (giving cross-validation error)\n",
    "    for d in days:\n",
    "        \n",
    "        #call the df_subset function to make numpy arrays out of the training and holdout data\n",
    "        X_T, y_T, X_CV, y_CV = make_numpy_arrays_for_tr_and_holdout(features, df[df.day != d], df[df.day == d], ref_column)\n",
    "                \n",
    "        MSE_CV_day, MSE_T_day = fitting_func(model, X_T, y_T, X_CV, y_CV)\n",
    "         \n",
    "        #record the MSE for lambda for the day\n",
    "        MSE_CV.append(MSE_CV_day)\n",
    "        MSE_T.append(MSE_T_day)\n",
    "    \n",
    "        #record the day\n",
    "        day_date.append(d)\n",
    "            \n",
    "        #find the mean MSE of all of the days for the given value of lambda\n",
    "        mean_CV_MSE_all_days = np.mean(MSE_CV)\n",
    "        mean_train_MSE_all_Days = np.mean(MSE_T)\n",
    "        \n",
    "    print \"Cross-Validation MSE: \", int(mean_CV_MSE_all_days), \" Training MSE: \", int(mean_train_MSE_all_Days)\n",
    "\n",
    "    return mean_CV_MSE_all_days, mean_train_MSE_all_Days "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Plot Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import learning_curve\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylimit, cv, train_sizes, scoring):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylimit is not None:\n",
    "        plt.ylim(ylimit)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "    train_sizes, train_scores, valid_scores = learning_curve(estimator, X, y, cv = cv, train_sizes = train_sizes, scoring = scoring)\n",
    "    train_scores_mean = -np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    valid_scores_mean = -np.mean(valid_scores, axis=1)\n",
    "    valid_scores_std = np.std(valid_scores, axis=1)\n",
    "    plt.grid(b=True, which='major', color='g', linestyle='-.')\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, valid_scores_mean - valid_scores_std, valid_scores_mean + valid_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training error\")\n",
    "    plt.plot(train_sizes, valid_scores_mean, 'o-', color=\"g\", label=\"Cross-validation error\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Define a function to find fitted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_fitted_cv_values_for_best_features(df, fs_features, num_good_feat, Model, days, ref_column):\n",
    "    fitted_holdout_o3 = []\n",
    "    for d in days:    \n",
    "        #call the df_subset function to make numpy arrays out of the training and holdout data\n",
    "        X_T, y_T, X_CV, y_CV = make_numpy_arrays_for_tr_and_holdout(fs_features[:num_good_feat], df[df.day != d], df[df.day == d], ref_column) \n",
    "        #fit a linear regression on the training data\n",
    "        model = Model\n",
    "        model.fit(X_T, y_T)\n",
    "        \n",
    "        if d == days[0]:\n",
    "            fitted_CV_o3 = model.predict(X_CV)\n",
    "        else:\n",
    "            fitted_CV_o3 = np.concatenate((fitted_CV_o3, model.predict(X_CV)))\n",
    "\n",
    "    df_lin_regr_best_feat = df.copy()\n",
    "    df_lin_regr_best_feat['O3_fit'] = fitted_CV_o3\n",
    "    return df_lin_regr_best_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Define functions for plotting fitted vs. holdout data and residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fitted_vs_ref_plot(df, i, ref_column):\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.plot(df[ref_column],df.O3_fit,linestyle = '',marker = '.',alpha = 0.3)\n",
    "    plt.xlabel('Reference O3 Conc.')\n",
    "    plt.ylabel('Predicted O3 Conc (Cross-Validation)')\n",
    "    plt.plot([1,df[ref_column].max()],[1,df[ref_column].max()])\n",
    "    if i != 0:\n",
    "        plt.title('Number of features = ' + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Define a function that assigns time chunks to each pod for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assign_pod_calibration_times(pod_num, time_chunk):\n",
    "    if time_chunk == 1:\n",
    "        if pod_num == 'D0' or pod_num == 'F3' or pod_num == 'F4' or pod_num == 'D3' or pod_num == 'F5' or pod_num == 'F6'  or pod_num == 'F7':\n",
    "            xlim = ['2014-07-11 00:00:00', '2014-07-13 00:00:00']\n",
    "        elif pod_num == 'D8' :\n",
    "            xlim = ['2014-07-11 00:00:00', '2014-7-12 00:00:00']\n",
    "        elif pod_num == 'D4' or pod_num == 'D6' or pod_num == 'D8' or pod_num == 'N4' or pod_num == 'N7' or pod_num == 'N8':\n",
    "            xlim = ['2014-07-13 00:00:00', '2014-7-15 00:00:00']\n",
    "        elif pod_num == 'N3' or pod_mun == 'N5':\n",
    "            xlim = ['2014-07-8 00:00:00', '2014-7-11 00:00:00']\n",
    "    else: \n",
    "        if pod_num == 'D0':\n",
    "            xlim = ['2014-08-30 00:00:00', '2014-09-4 00:00:00']\n",
    "        elif pod_num == 'D4' or pod_num == 'F4':\n",
    "            xlim = ['2014-08-15 00:00:00', '2014-08-21 00:00:00']\n",
    "        elif pod_num == 'D3' or pod_num == 'D6' or pod_num == 'F3' or pod_num == 'D8' or pod_num == 'F5' or pod_num == 'F6' or pod_num == 'N8':\n",
    "            xlim = ['2014-08-21 00:00:00', '2014-08-30 00:00:00']\n",
    "        elif pod_num == 'F7' or pod_num == 'N4':\n",
    "            xlim = ['2014-08-15 00:00:00', '2014-08-21 00:00:00']\n",
    "        elif pod_num == 'N3':\n",
    "            xlim = ['2014-08-14 00:00:00', '2014-08-21 00:00:00']\n",
    "        elif pod_num == 'D4' or pod_num == 'N5':\n",
    "            xlim = ['2014-08-29 00:00:00', '2014-09-4 00:00:00']\n",
    "        elif pod_num == 'N7':\n",
    "            xlim = ['2014-08-16 00:00:00', '2014-08-22 00:00:00']\n",
    "    return xlim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_fitted_and_ref_vs_time(df, pod_num, time_chunk, ref_column):\n",
    "    plt.figure(figsize = (15,10))\n",
    "    df[ref_column].plot(marker = '.',linestyle = ' ',)\n",
    "    xlim = assign_pod_calibration_times(pod_num, time_chunk)\n",
    "    df.O3_fit.plot(marker = '.',linestyle = ' ', xlim=xlim)\n",
    "    plt.ylabel('Residual Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_resid_vs_conc(df, ref_column):\n",
    "    #find the residuals\n",
    "    resid = df[ref_column] - df.O3_fit\n",
    "    #plot the residuals to check for non-linearity of response predictor\n",
    "    plt.figure(figsize = (15,5))\n",
    "    plt.plot(df.O3_fit, resid, linestyle = '',marker = '.',alpha = 0.4)\n",
    "    plt.plot([-40,70],[0,0], linestyle = ' ', marker = '.')\n",
    "    plt.xlabel('Fitted O3 Conc.')\n",
    "    plt.ylabel('Residuals')\n",
    "    return resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_resid_vs_time(resid, pod_num, time_chunk):\n",
    "    plt.figure(figsize = (15,5))\n",
    "    xlim = assign_pod_calibration_times(pod_num, time_chunk)\n",
    "    resid.plot(linestyle = '',marker = '.', xlim = xlim)\n",
    "    #plt.plot([0,0],[70,0])\n",
    "    plt.xlabel('Fitted O3 Conc.')\n",
    "    plt.ylabel('Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Make custom scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "def custom_mse_scoring_function(y, y_pred):\n",
    "    low_sum = np.mean(0.1*(y[y < 60] - y_pred[y < 60])**2)\n",
    "    high_sum = np.mean((y[y >= 60] - y_pred[y >= 60])**2)\n",
    "    if np.isnan(low_sum) == True:\n",
    "        low_sum = 0\n",
    "    if np.isnan(high_sum) == True:\n",
    "        high_sum = 0\n",
    "    return int(low_sum + high_sum)\n",
    "\n",
    "def custom_mae_scoring_function(y, y_pred):\n",
    "    low_sum = np.mean(0.1*np.absolute(y[y < 65] - y_pred[y < 65]))\n",
    "    high_sum = np.mean(np.absolute(y[y >= 65] - y_pred[y >= 65]))\n",
    "    if np.isnan(low_sum) == True:\n",
    "        low_sum = 0\n",
    "    if np.isnan(high_sum) == True:\n",
    "        high_sum = 0\n",
    "    return int(low_sum + high_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Find the average score for all days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avg_cv_score_for_all_days(df, features, ref_column, model, scoring_metric,lol):\n",
    "    X = df[features].values\n",
    "    y = df[ref_column].values\n",
    "    if scoring_metric == 'custom_mse':\n",
    "        score_cv = -np.mean(cross_val_score(model, X, y, cv = lol, scoring = make_scorer(custom_mse_scoring_function, greater_is_better = False)))        \n",
    "    elif scoring_metric == 'custom_mae':\n",
    "        score_cv = -np.mean(cross_val_score(model, X, y, cv = lol, scoring = make_scorer(custom_mae_scoring_function, greater_is_better = False)))        \n",
    "    else:\n",
    "        score_cv = -np.mean(cross_val_score(model, X, y, cv = lol, scoring = scoring_metric))\n",
    "    return score_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_selection_step(model, b_f, features, df, ref_column, scoring_metric, lol):\n",
    "    #initialize min_MSE with a very large number\n",
    "    min_score = sys.maxint\n",
    "    min_r2 = 0\n",
    "    next_feature = ''\n",
    "\n",
    "    for f in features:\n",
    "        score_step = avg_cv_score_for_all_days(df, b_f + [f], ref_column, model, scoring_metric, lol)\n",
    "        if score_step < min_score:\n",
    "            min_score = score_step\n",
    "            next_feature = f\n",
    "            score_cv = \"{:.1f}\".format(min_score)   \n",
    "    return next_feature, score_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_selection_lodo(model, features, df, scoring_metric, ref_column, lol):\n",
    "    #initialize the best_features list with the base features to force their inclusion\n",
    "    best_features = ['days from start']\n",
    "    #call the function that scales the features and creates a day column   \n",
    "    \n",
    "    score_cv = []\n",
    "    MSE = []\n",
    "    while len(features) > 0 and len(best_features) < 51:\n",
    "        #next_features = []\n",
    "        #score_cv_list = []\n",
    "             \n",
    "        next_feature, score_cv_feat = forward_selection_step(model, best_features, features, df, ref_column, scoring_metric, lol)\n",
    "        #add the next feature to the list\n",
    "        best_features += [next_feature]\n",
    "        MSE.append(\"{:.1f}\".format(-np.mean(cross_val_score(model, df[best_features].values, df[ref_column].values, cv = lol, scoring = 'mean_squared_error'))))\n",
    "        score_cv.append(score_cv_feat)\n",
    "        print 'Next best Feature: ', next_feature, ',', 'Score: ', score_cv_feat\n",
    "        \n",
    "        #remove the added feature from the list\n",
    "        features.remove(next_feature)\n",
    "        \n",
    "    print \"Best Features: \", best_features\n",
    "    return best_features, score_cv, MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Plot the custom error and MSE as a function of number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_error_vs_features(score, MSE):\n",
    "    x = range(0, len(score))\n",
    "    plt.plot(x, score, 'bo-')\n",
    "    plt.plot(x, MSE, 'ro-')\n",
    "    plt.ylim((0,100))\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('Error')\n",
    "    plt.grid(b=True, which='major', color='g', linestyle='-.')\n",
    "    print 'Custom Score: ', score\n",
    "    print 'MSE: ', MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#__all__ = [\"echo\", \"surround\", \"reverse\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    fib(int(sys.argv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
